{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Project submitted by:__<br>\n",
    "__Divij Jasuja (24954)__<br>\n",
    "__Pranay Raturi (24920)__<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does twitter affect stock prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Problem Statement](#problem-statement)\n",
    "2. [Libraries used](#libraries-used)\n",
    "3. [Collection of data](#collection-of-data)<br>\n",
    "    3.1. [Collecting stock data]()<br>\n",
    "    3.2. [Collecting tweets]()\n",
    "4. [Data cleaning](#data-cleaning)\n",
    "5. [Sentiment Analysis](#sentiment-analysis)\n",
    "6. [Visualizing the correlation between twitter sentiment and stock performance](#vizualization)\n",
    "7. [Applying LSTM](#applying-lstm) \n",
    "8. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project aims to utilize sentiment analysis techniques on tweets related to NVIDIA Corporation (NASDAQ: NVDA) to explore and establish correlations between the sentiments expressed in these tweets and the subsequent stock performance of NVIDIA. The goal is to leverage these findings to develop a predictive model that can assist in forecasting stock movements based on sentiment trends in social media.<br><br>\n",
    "The objectives of the project are as follows:\n",
    "- Gather historical stock performance data for NVIDIA during Q1 2024.\n",
    "- Collect a comprehensive dataset of tweets mentioning NVIDIA or related keywords over the same period.\n",
    "- Implement sentiment analysis algorithms to quantify the sentiments expressed in these tweets as positive negative, or neutral.\n",
    "- Conduct statistical analysis to identify correlations between sentiment trends in tweets and subsequent stock price movements.\n",
    "- Use the findings to develop a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For general data handling \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# For fetching stock data and tweets\n",
    "import yfinance as yf\n",
    "from twscrape import API\n",
    "\n",
    "# For processing textual data\n",
    "import demoji\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# For sentiment analysis\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For LSTM \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasoning behind the selection of certain libraries will be explained later in the project, when said libraries are utilized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collection of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collecting stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get ticker 'NVDA' reason: Expecting value: line 1 column 1 (char 0)\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['NVDA']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data returned for ticker: NVDA\n"
     ]
    }
   ],
   "source": [
    "# NVDA is the stock ticker of Nvidia on the NASDAQ\n",
    "ticker = \"NVDA\"\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = \"2024-03-31\"\n",
    "\n",
    "try:\n",
    "    # download the data from Yahoo Finance\n",
    "    data = yf.download(ticker, start=start_date, end=end_date) \n",
    "    if data.empty:\n",
    "        print(f'No data returned for ticker: {ticker}')\n",
    "    else:\n",
    "        # convert the downloaded data into csv files\n",
    "        data.to_csv(f\"stock_data/{ticker}_{start_date[-5:]}-{end_date[-5:]}\")\n",
    "        print(\"Data added to stock_data folder\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data for {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collecting tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mine the tweets, the Python library **twscrape** was used. The library works through an authorised API, therefore twitter account(s) are needed in order to use it. A twitter username and password, and also the email associated with the twitter account and its password are needed to be able to collect tweets.\n",
    "\n",
    "The library is designed to automatically switch accounts when the twitter API limit has been reached per 15-minute interval. So multiple accounts can be added to the API pool in order to change to a different account and continue scraping when the other accounts have reached their API limits.\n",
    "\n",
    "The library makes use of **asynchronous programming**, allowing the program to continue executing even while some of the accounts have reached their API limits.\n",
    "\n",
    "More information about this library can be found in [this medium article](https://medium.com/@vladkens/how-to-still-scrape-millions-of-tweets-in-2023-using-twscrape-97f5d3881434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = API()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The search range in the query spans over twelve weeks (from 2024-01-01 to 2024-01-04),for the program to fetch all the tweets at once, it would take a long time(-about 12 hours). To avoid this, a function was created which would take in the start date, end date and the directory to save the output, and return a dataframe of the scraped tweets.\n",
    "\n",
    "The function is defined using async def. This is because twscrape uses a __coroutine function__ to scrape tweets, so using the regular def for defining regular python functions will not work. i.e. async def is used to define coroutine functions in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_tweets(company, start_date, end_date):\n",
    "    \n",
    "    data = []  #create an empty list to be used to store the search results\n",
    "    \n",
    "    #define the search query. Include start date and end date\n",
    "    q = f\"${company} until:{end_date} since:{start_date} lang:en\" \n",
    "    save_to_file = f\"twitter_data/{company}_{start_date}-{end_date}\"\n",
    "    \n",
    "    async for tweet in api.search(q, limit=300000): #iterate over the search results\n",
    "        c = [tweet.id, tweet.date, tweet.rawContent, tweet.likeCount, tweet.retweetCount, tweet.user.location] #list of attributes to return \n",
    "        data.append(c)  #add each new list of attributes to 'data'\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['Tweet_ID', 'Time_Created', 'Text', 'Likes', 'Retweets', 'Location']) #convert the list to a dataframe\n",
    "    df.to_csv(save_to_file, index = False) #save to a chosen directory on the computer\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of how the function works\n",
    "nvidia_tweets = await scrape_tweets(\"NVDA\", \"2024-01-01\", \"2024-01-02\")\n",
    "nvidia_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an identifier containing the path to the folder\n",
    "folder = \"twitter_data\"\n",
    "\n",
    "# Read each file into a dataframe and store them in a list\n",
    "dfs = []\n",
    "count = 0\n",
    "for file_name in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "    \n",
    "# Merge the dataframes \n",
    "tweets = pd.concat(dfs, axis=0, ignore_index=True)  \n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "tweets = tweets.drop_duplicates(subset=['Tweet_ID'], keep='first').reset_index(drop=True)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the number of missing values in each column of the dataset\n",
    "tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location attribute was not very useful since around 50% of its values were missing. Additionaly, location is just a string entered by the users and is not always an actual place. Many enteries in this dataframe also had location attribute values like \"Milky Way\" and other jokes. For these reasons, the location attribute was dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the location attribute\n",
    "tweets  = tweets.drop(\"Location\", axis = 1)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __'Time_Created'__ attribute contained a timestamp in the ISO 8601 standard notation. It contained information like, time and UTC offset which were not required for this project. Hence, the attribute was converted to dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the time created attribute to dates \n",
    "tweets['Time_Created'] = pd.to_datetime(tweets['Time_Created']).dt.date\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any tweets that were fetched but lied outside the dersired range were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end dates of the range\n",
    "start_date = date(2024, 1, 1)\n",
    "end_date = date(2024, 3, 31)\n",
    "# Filter the DataFrame based on the date range\n",
    "filtered_tweets = tweets[(tweets['Time_Created'] >= start_date) & (tweets['Time_Created'] <= end_date)]\n",
    "filtered_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to pass the POS tag for each word passed through clean_text function\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function was used to clean the tweets and make them more suitable to be used as an input in the sentiment analysis model. The function makes use of libraries like __re__ (A python library used for regular expressions) and __nltk__ (A python library used for NLP tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning tweets\n",
    "def clean_text(text):\n",
    "    # Initialization the twitter tokenizer\n",
    "    tk = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True) \n",
    "    # Initialization the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    # Trying to avoid deleting the negative verbs as it affects the meaning of the tweets.\n",
    "    stop_words = stopwords.words('english') + [\"i'll\",\"i'm\", \"should\", \"could\"]\n",
    "    negative_verbs = [ \"shan't\",'shouldn',\"shouldn't\",'wasn','weren','won','wouldn','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn','mustn',\"mustn't\",'needn',\"needn't\",\"wouldn't\",\"won't\",\"weren't\",\"wasn't\",\"couldn\",\"not\",\"nor\",\"no\",\"mightn't\",\"isn't\",\"haven't\",\"hadn't\",\"hasn't\",\"didn't\",\"doesn't\",\"aren't\",\"don't\",\"couldn't\",\"never\"]\n",
    "    stop_words =[word for word in stop_words if word not in negative_verbs ] \n",
    "    \n",
    "    # Lowering tweets\n",
    "    lower_tweet = text.lower() \n",
    "    # Removing hashtag and cashtag symbols\n",
    "    tweet = re.sub(r\"[#$]\",\" \",lower_tweet)\n",
    "    # Removing links from tweets\n",
    "    tweet = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\",\" \", tweet)\n",
    "    # Translating emojies into thier descriptions\n",
    "    tweet = demoji.replace_with_desc(tweet)\n",
    "    # removing numerical values\n",
    "    tweet = re.sub(r\"[0-9]|-->\",\"\",tweet)\n",
    "    # Tokenize the tweets by twitter tokenzier.\n",
    "    tweet = tk.tokenize(tweet)\n",
    "    # Choosing the words that don't exist in stopwords, thier lengths are more than 2 letters and then lemmatize them.\n",
    "    tweet = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tweet if word not in stop_words and word not in string.punctuation and len(word)>2 and \".\" not in word]\n",
    "    # return the tokens in one sentence \n",
    "    tweet = \" \".join(tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying text cleaning \n",
    "filtered_tweets['cleaned'] = filtered_tweets[\"Text\"].apply(lambda row:clean_text(row))\n",
    "# Sorting the dataframe based on 'Time_Created'\n",
    "filtered_tweets.sort_values(by = 'Time_Created', inplace = True)\n",
    "# Saving to a csv file\n",
    "filtered_tweets.to_csv(\"twitter_data/NVDA_final-tweets\")\n",
    "print(\"Filtered tweets added to the folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RoBERTa__ stands for \"A Robustly Optimized BERT Pretraining Approach.\" It's a variant of the BERT (Bidirectional Encoder Representations from Transformers). The reasons it is used commonly for sentiment analysis are:\n",
    "-  RoBERTa's ability to understand context and capture nuanced semantics makes it effective for sentiment analysis tasks where context plays a crucial role in determining sentiment (e.g., understanding sarcasm, negation, or sentiment shift within a sentence).\n",
    "- RoBERTa can be fine-tuned on sentiment analysis datasets, where the model learns to predict sentiment labels (e.g., positive, negative, neutral) based on text inputs. Fine-tuning allows RoBERTa to adapt its pre-trained knowledge to specific sentiment analysis tasks, leading to improved accuracy and performance.\n",
    "\n",
    "The model that has been used for sentiment analysis is the Twitter roBERTa model that is available at this [link](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)<br>\n",
    "This model that been trained on over 124 million tweets from the time period of January 2018 to December 2021, making it a great fit for the project \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Softmax__ function is a mathematical function that converts a vector of numbers into a probability distribution.\n",
    "Given an input vector z=[$z_1$,$z_2$,...,$z_n$] , the softmax function computes the probability $p_i$​ for each element $z_i$ as follows:\n",
    "$$ p_i = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}} $$\n",
    "\n",
    "Where:\n",
    "- e is the base of the natural logarithm.\n",
    "- $z_i$ is the i-th element of the input vector.\n",
    "- $​{\\sum_{j=1}^{n} e^{z_j}}$ is the sum of the exponentiated values of all elements in the input vector.\n",
    "\n",
    "<br>It has been used here as the activation function because:\n",
    "- The softmax function is well-suited for sentiment analysis because it produces output probabilities that represent the likelihood of each class, allowing the model to make predictions across multiple classes.\n",
    "- Softmax ensures that the output probabilities sum up to 1, forming a valid probability distribution.\n",
    "- Softmax is a differentiable function, which means that gradients can be computed with respect to its inputs. This property is crucial for training the RoBERTa model using gradient-based optimization algorithms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True, max_length=512)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a sample run\n",
    "print(f'For a positive statement : {sentiment_analysis(\"I like these stocks\")}')\n",
    "print(f'For a neutral statement : {sentiment_analysis(\"I do not know about these stocks\")}')\n",
    "print(f'For a negative statement : {sentiment_analysis(\"I hate these stocks\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function was then applied to all the tweets in the data set with the following code:\n",
    "<pre>\n",
    "#tqdm was used here to get a progress bar for the sentiment analysis\n",
    "tqdm.pandas()\n",
    "filtered_tweets['cleaned'] = tweets['cleaned'].fillna(\"\")\n",
    "tweets['sentiment'] = tweets['cleaned'].progress_apply(lambda x: sentiment_analysis(x))\n",
    "tweets.to_csv('analysed_tweets.csv')</pre>\n",
    "The code has not been executed in the notebook as its execution takes around 6 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"twitter_data/analysed_tweets.csv\", index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the use-case of this project, it would be more suitable if the sentiment was represented by:<br>\n",
    "{Postive : 1, Neutral : 0, Negative : -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].apply(lambda x: x - 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.DataFrame(df.groupby(\"Time_Created\")['sentiment'].mean())\n",
    "sentiment_df.rename(columns = {\"sentiment\":\"sen_mean\"}, inplace = True)\n",
    "sentiment_df['twt_volume'] = df.groupby(['Time_Created'])['sentiment'].count()\n",
    "sentiment_df['sen_sum'] = df.groupby('Time_Created')['sentiment'].sum()\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_stocks = pd.read_csv(\"stock_data/NVDA_01-01-03-31\")\n",
    "nvda_stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(nvda_stocks, sentiment_df, left_on = \"Date\", right_on = \"Time_Created\", how = 'inner')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the data in a csv file\n",
    "final_df.to_csv(\"NVDA_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __candlestick__ graph representing the performance of NVDA in Q1 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = go.Figure(data=[go.Candlestick(x= nvda_stocks['Date'],\n",
    "                open= nvda_stocks['Open'],\n",
    "                high= nvda_stocks['High'],\n",
    "                low= nvda_stocks['Low'],\n",
    "                close= nvda_stocks['Close'])])\n",
    "fig.update_layout(title = {'text' : \"Nvidia(NVDA) Q1 2024\",\n",
    "                           'y':0.9,\n",
    "                           'x':0.5,\n",
    "                           'xanchor': 'center',\n",
    "                           'yanchor': 'top'},\n",
    "                  xaxis_title = \"Dates\",\n",
    "                  yaxis_title = \"Price\",\n",
    "                  xaxis_rangeslider_visible = False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __correlation__ between __volume of tweets__ and __volume of trades__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "# Volume was divided by 10000 to make the ranges of the attributes similar\n",
    "sns.lineplot(data=final_df, x='Date', y=final_df['Volume']/10000, label='Volume')\n",
    "sns.lineplot(data=final_df, x='Date', y=final_df['twt_volume'], label='Tweet Volume')\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "plt.title('Line Plots of Stock Volume and Tweet Volume')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize = 15)\n",
    "pearson_corr = final_df['Volume'].corr(final_df['twt_volume'], method='pearson')\n",
    "\n",
    "# Display correlation coefficients on the plot\n",
    "plt.annotate(f\"Pearson Correlation: {pearson_corr:.2f}\", xy=(0.25, 0.9), xycoords='axes fraction', ha='center', fontsize=15, color='green')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tweets about NVIDIA on a specific day have a __very high correlation__ with the number of stock traded for NVDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __correlation__ between __mean sentiment__ and __stock close__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "sns.lineplot(data=final_df, x='Date', y=final_df['Close'], label='Close')\n",
    "sns.lineplot(data=final_df, x='Date', y=final_df['sen_sum'], label='Tweet Sentiment')\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Line Plots of Stock Volume and Tweet Volume')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize = 15)\n",
    "pearson_corr = final_df['Close'].corr(final_df['sen_sum'], method='pearson')\n",
    "\n",
    "# Display correlation coefficients on the plot\n",
    "plt.annotate(f\"Pearson Correlation: {pearson_corr:.2f}\", xy=(0.1, 0.80), xycoords='axes fraction', ha='center', fontsize=15, color='green')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between twitter sentiment and the closing price of the stock is very low. This suggests that the twitter sentiment will not significantly improve the performance of the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep learning model that has been used in this project for stock prediction is __LSTM (Long Short-Term Memory)__.<br>\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that is designed to overcome the limitations of traditional RNNs in capturing long-range dependencies and handling vanishing or exploding gradients. LSTMs are widely used in various sequence modeling tasks, including time series analysis, and financial forecasting, such as stock analysis.<br>\n",
    "\n",
    "The advantages that LSTM provides for stock prediction are as follows:\n",
    "- LSTMs are equipped with memory cells that allow them to remember information over long sequences. This is crucial for analyzing time series data like stock prices, where past prices and trends can have a significant impact on future movements.\n",
    "-  LSTMs are well-suited for capturing time dependencies and learning patterns in sequential data.\n",
    "-  LSTMs address the issue of vanishing gradient by using gating mechanisms (such as the forget gate, input gate, and output gate) to regulate the flow of information and gradients within the network.\n",
    "- LSTMs are adaptable and can be customized based on the specific requirements of the stock analysis task. For example, the network architecture, hyperparameters, and training data can be adjusted to optimize performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_symbol(x):\n",
    "    if x == 1:\n",
    "        return 'pos'\n",
    "    elif x == 0:\n",
    "        return 'nue'\n",
    "    else:\n",
    "        return 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates = pd.to_datetime(final_df['Date'])\n",
    "\n",
    "#Variables for training\n",
    "cols = [\n",
    "    'Open',\n",
    "    'High', 'Low',\n",
    "    'Close',\n",
    "    'Volume',\n",
    "    'Adj Close',\n",
    "    'sen_mean',\n",
    "    'twt_volume'\n",
    "        ]\n",
    "#Date and volume columns are not used in training.\n",
    "\n",
    "#New dataframe with only training data - 5 columns\n",
    "df_for_training = final_df[cols].astype(float)\n",
    "df_for_training.index=final_df['Date']\n",
    "df_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)\n",
    "\n",
    "scaler_for_inference = MinMaxScaler()\n",
    "scaler_for_inference.fit_transform(df_for_training.loc[:,['Open','Adj Close']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_training_scaled.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "n_past = 5  # Number of past days we want to use to predict the future.\n",
    "\n",
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future,[0,-2]])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print(f'TrainX shape = {trainX.shape}')\n",
    "print(f'TrainY shape = {trainY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_lstm_without_twitter, X_test_lstm_without_twitter, y_train_lstm_without_twitter, y_test_lstm_without_twitter = train_test_split(trainX[:,:,:-2], trainY, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train_lstm_twitter, X_test_lstm_twitter, y_train_lstm_twitter, y_test_lstm_twitter = train_test_split(trainX, trainY, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train_lstm_without_twitter.shape,X_train_lstm_twitter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock Prediction with twitter sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(X_train_lstm_twitter).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(X_test_lstm_twitter).type(torch.Tensor)\n",
    "y_train_lstm = torch.from_numpy(y_train_lstm_twitter).type(torch.Tensor)\n",
    "y_test_lstm = torch.from_numpy(y_test_lstm_twitter).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 8\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "num_epochs = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "for t in range(num_epochs):\n",
    "    y_train_pred = model(x_train)\n",
    "    loss = criterion(y_train_pred, y_train_lstm)\n",
    "    # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "print(f'MSE for training: {loss.item():.3f}')\n",
    "training_time = time.time()-start_time\n",
    "print(f\"Training time: {training_time:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 16 \n",
    "moving_avg = np.convolve(hist, np.ones(window_size)/window_size, mode='valid')\n",
    "sns.lineplot(x = range(window_size, num_epochs + 1), y = moving_avg, label = f'Moving Average (Window Size {window_size})')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs. Number of Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model(x_test)\n",
    "loss = criterion(y_test_pred, y_test_lstm)\n",
    "print(f'MSE for testing with twitter: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock prediction without twitter sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(X_train_lstm_without_twitter).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(X_test_lstm_without_twitter).type(torch.Tensor)\n",
    "y_train_lstm = torch.from_numpy(y_train_lstm_without_twitter).type(torch.Tensor)\n",
    "y_test_lstm = torch.from_numpy(y_test_lstm_without_twitter).type(torch.Tensor)\n",
    "input_dim = 6\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "num_epochs = 512\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "import time\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "for t in range(num_epochs):\n",
    "    y_train_pred = model(x_train)\n",
    "    loss = criterion(y_train_pred, y_train_lstm)\n",
    "    hist[t] = loss.item()\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "print(f'MSE for training: {loss.item():.3f}')\n",
    "training_time = time.time()-start_time\n",
    "print(f\"Training time: {training_time:.3f}\")\n",
    "y_test_pred = model(x_test)\n",
    "loss = criterion(y_test_pred, y_test_lstm)\n",
    "print(f'MSE for testing without twitter: {loss.item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 16  # Adjust window size as needed\n",
    "moving_avg = np.convolve(hist, np.ones(window_size)/window_size, mode='valid')\n",
    "sns.lineplot(x = range(window_size, num_epochs + 1), y = moving_avg, label = f'Moving Average (Window Size {window_size})')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs. Number of Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The testing error for the prediction is very small (0.042). However as mentioned previously in the visualization section, given the low correlation between the twitter sentiment and the closing price, the twitter sentiment fails to meaningfully improve the predictions made by the LSTM model.<br>\n",
    "- The project is limited to only the first quater of 2024 of only one company, the improvements from the sentiment maybe more noticeable for different companies or for NVIDIA over a longer period of time.<br>\n",
    "- Further research, with a wider scope needs to be performed to definitively prove or disprove the viability of using twitter sentiment to predict stock movements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
