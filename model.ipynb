{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Stock Prediction Model\n",
    "\n",
    "This project made by Divij Jasuja and Pranay Raturi for Data Mining Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached torch-2.3.0-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting torchmetrics\n",
      "  Using cached torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in .\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: fsspec in .\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in .\\.venv\\lib\\site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in .\\.venv\\lib\\site-packages (from torchmetrics) (24.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in .\\.venv\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.5.0)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.3.0-cp311-cp311-win_amd64.whl (159.8 MB)\n",
      "Using cached torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Using cached tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, sympy, networkx, mkl, MarkupSafe, lightning-utilities, jinja2, torch, torchmetrics\n",
      "Successfully installed MarkupSafe-2.1.5 intel-openmp-2021.4.0 jinja2-3.1.3 lightning-utilities-0.11.2 mkl-2021.4.0 mpmath-1.3.0 networkx-3.3 sympy-1.12 tbb-2021.12.0 torch-2.3.0 torchmetrics-1.3.2\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.4.28-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in .\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\.venv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "Using cached regex-2024.4.28-cp311-cp311-win_amd64.whl (268 kB)\n",
      "Using cached safetensors-0.4.3-cp311-none-win_amd64.whl (287 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Installing collected packages: safetensors, regex, pyyaml, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.14.0 fsspec-2024.3.1 huggingface-hub-0.22.2 pyyaml-6.0.1 regex-2024.4.28 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.40.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\sentiment analysis and stock prediction\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is LABEL_2 with a confidence of 0.9292691349983215\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the sentiment analysis pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Use the pipeline to analyze the sentiment of a text\n",
    "text = \"nvidia is a good company\"\n",
    "result = nlp(text)\n",
    "\n",
    "# The result is a list of dictionaries. Each dictionary contains the 'label' and 'score'.\n",
    "# 'label' is the sentiment (POSITIVE or NEGATIVE), and 'score' is the confidence.\n",
    "sentiment = result[0]['label']\n",
    "confidence = result[0]['score']\n",
    "\n",
    "print(f\"The sentiment is {sentiment} with a confidence of {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"stocks are ok\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "scores.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Time_Created</th>\n",
       "      <th>Text</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1744146496243822931</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>@SelfMadeMastery Best: $NVDA, $CRWD, $META, $T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>best nvda crwd meta tsla bad enph use oppurtun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1744146280576926128</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>Most Notable #Earnings Week of JAN 8th\\n\\nâ—¦ Mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>notable earnings week jan mon accd tues tlry a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1744145386850422822</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ U.S. ECONOMIC DATA 2ND WEEK\\n\\nTHURS.\\nâ—¦ U....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>flag united state economic data week thurs cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1744145096592007411</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>$MARA We nailed this play. I kept on hammering...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>mara nail play kept hammer risk reward hence l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1743200305033175350</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>$MARA Scaled out of this position yesterday\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>mara scale position yesterday last time specul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             Tweet_ID Time_Created  \\\n",
       "0           0  1744146496243822931   2024-01-07   \n",
       "1           1  1744146280576926128   2024-01-07   \n",
       "2           2  1744145386850422822   2024-01-07   \n",
       "3           3  1744145096592007411   2024-01-07   \n",
       "4           4  1743200305033175350   2024-01-05   \n",
       "\n",
       "                                                Text  Likes  Retweets  \\\n",
       "0  @SelfMadeMastery Best: $NVDA, $CRWD, $META, $T...      1         0   \n",
       "1  Most Notable #Earnings Week of JAN 8th\\n\\nâ—¦ Mo...      0         0   \n",
       "2  ðŸ‡ºðŸ‡¸ U.S. ECONOMIC DATA 2ND WEEK\\n\\nTHURS.\\nâ—¦ U....      0         0   \n",
       "3  $MARA We nailed this play. I kept on hammering...      3         0   \n",
       "4  $MARA Scaled out of this position yesterday\\n\\...      1         0   \n",
       "\n",
       "                                             cleaned  \n",
       "0  best nvda crwd meta tsla bad enph use oppurtun...  \n",
       "1  notable earnings week jan mon accd tues tlry a...  \n",
       "2  flag united state economic data week thurs cpi...  \n",
       "3  mara nail play kept hammer risk reward hence l...  \n",
       "4  mara scale position yesterday last time specul...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv('twitter_data/NVDA_final-tweets')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True, max_length=512)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 269199/269199 [5:52:34<00:00, 12.73it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "tweets['cleaned'] = tweets['cleaned'].fillna(\"\")\n",
    "tweets['sentiment'] = tweets['cleaned'].progress_apply(lambda x: sentiment_analysis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('analysed_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Time_Created</th>\n",
       "      <th>Text</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1741926827688681728</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>Our analyst called the PUMP on $FLJ, securing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>analyst call pump flj secure free trial member...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1741855632767004892</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>Stocks Performance Upto 1Y\\n\\nEverything is aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stock performance upto everything awesome tsla...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1741856010761802082</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>Were you invested in $NVDA pre 2021? \\n\\nIf th...</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>invest nvda pre answer yes see make think good...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1741856822917513633</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>Top 3 Bearish Sentiment Cryptos: CROWD\\n\\n ðŸŸ¥ $...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>top bearish sentiment cryptos crowd red square...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1741856927926100073</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>$nvda Top analyst price target for next week:....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nvda top analyst price target next week</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             Tweet_ID Time_Created  \\\n",
       "0           0  1741926827688681728   2024-01-01   \n",
       "1           1  1741855632767004892   2024-01-01   \n",
       "2           2  1741856010761802082   2024-01-01   \n",
       "3           3  1741856822917513633   2024-01-01   \n",
       "4           4  1741856927926100073   2024-01-01   \n",
       "\n",
       "                                                Text  Likes  Retweets  \\\n",
       "0  Our analyst called the PUMP on $FLJ, securing ...      0         0   \n",
       "1  Stocks Performance Upto 1Y\\n\\nEverything is aw...      0         0   \n",
       "2  Were you invested in $NVDA pre 2021? \\n\\nIf th...     14         2   \n",
       "3  Top 3 Bearish Sentiment Cryptos: CROWD\\n\\n ðŸŸ¥ $...      0         0   \n",
       "4  $nvda Top analyst price target for next week:....      0         0   \n",
       "\n",
       "                                             cleaned  sentiment  \n",
       "0  analyst call pump flj secure free trial member...          1  \n",
       "1  stock performance upto everything awesome tsla...          2  \n",
       "2  invest nvda pre answer yes see make think good...          2  \n",
       "3  top bearish sentiment cryptos crowd red square...          1  \n",
       "4            nvda top analyst price target next week          1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"twitter_data/analysed_tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen_mean</th>\n",
       "      <th>twt_volume</th>\n",
       "      <th>sen_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time_Created</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>1.166667</td>\n",
       "      <td>636</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>1.083774</td>\n",
       "      <td>1325</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>1.102433</td>\n",
       "      <td>1562</td>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-04</th>\n",
       "      <td>1.138872</td>\n",
       "      <td>1649</td>\n",
       "      <td>1878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05</th>\n",
       "      <td>1.143971</td>\n",
       "      <td>2181</td>\n",
       "      <td>2495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sen_mean  twt_volume  sen_sum\n",
       "Time_Created                               \n",
       "2024-01-01    1.166667         636      742\n",
       "2024-01-02    1.083774        1325     1436\n",
       "2024-01-03    1.102433        1562     1722\n",
       "2024-01-04    1.138872        1649     1878\n",
       "2024-01-05    1.143971        2181     2495"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df = pd.DataFrame(df.groupby(\"Time_Created\")['sentiment'].mean())\n",
    "sentiment_df.rename(columns = {\"sentiment\":\"sen_mean\"}, inplace = True)\n",
    "sentiment_df['twt_volume'] = df.groupby(['Time_Created'])['sentiment'].count()\n",
    "sentiment_df['sen_sum'] = df.groupby('Time_Created')['sentiment'].sum()\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>492.440002</td>\n",
       "      <td>492.950012</td>\n",
       "      <td>475.950012</td>\n",
       "      <td>481.679993</td>\n",
       "      <td>481.657410</td>\n",
       "      <td>41125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>474.850006</td>\n",
       "      <td>481.839996</td>\n",
       "      <td>473.200012</td>\n",
       "      <td>475.690002</td>\n",
       "      <td>475.667694</td>\n",
       "      <td>32089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>477.670013</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>475.079987</td>\n",
       "      <td>479.980011</td>\n",
       "      <td>479.957489</td>\n",
       "      <td>30653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>484.619995</td>\n",
       "      <td>495.470001</td>\n",
       "      <td>483.059998</td>\n",
       "      <td>490.970001</td>\n",
       "      <td>490.946960</td>\n",
       "      <td>41456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>495.119995</td>\n",
       "      <td>522.750000</td>\n",
       "      <td>494.790009</td>\n",
       "      <td>522.530029</td>\n",
       "      <td>522.505493</td>\n",
       "      <td>64251000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2024-01-02  492.440002  492.950012  475.950012  481.679993  481.657410   \n",
       "1  2024-01-03  474.850006  481.839996  473.200012  475.690002  475.667694   \n",
       "2  2024-01-04  477.670013  485.000000  475.079987  479.980011  479.957489   \n",
       "3  2024-01-05  484.619995  495.470001  483.059998  490.970001  490.946960   \n",
       "4  2024-01-08  495.119995  522.750000  494.790009  522.530029  522.505493   \n",
       "\n",
       "     Volume  \n",
       "0  41125400  \n",
       "1  32089600  \n",
       "2  30653500  \n",
       "3  41456800  \n",
       "4  64251000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvda_stock = pd.read_csv(\"stock_data/NVDA_01-01-03-31\")\n",
    "nvda_stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>sen_mean</th>\n",
       "      <th>twt_volume</th>\n",
       "      <th>sen_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>492.440002</td>\n",
       "      <td>492.950012</td>\n",
       "      <td>475.950012</td>\n",
       "      <td>481.679993</td>\n",
       "      <td>481.657410</td>\n",
       "      <td>41125400</td>\n",
       "      <td>1.083774</td>\n",
       "      <td>1325</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>474.850006</td>\n",
       "      <td>481.839996</td>\n",
       "      <td>473.200012</td>\n",
       "      <td>475.690002</td>\n",
       "      <td>475.667694</td>\n",
       "      <td>32089600</td>\n",
       "      <td>1.102433</td>\n",
       "      <td>1562</td>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>477.670013</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>475.079987</td>\n",
       "      <td>479.980011</td>\n",
       "      <td>479.957489</td>\n",
       "      <td>30653500</td>\n",
       "      <td>1.138872</td>\n",
       "      <td>1649</td>\n",
       "      <td>1878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>484.619995</td>\n",
       "      <td>495.470001</td>\n",
       "      <td>483.059998</td>\n",
       "      <td>490.970001</td>\n",
       "      <td>490.946960</td>\n",
       "      <td>41456800</td>\n",
       "      <td>1.143971</td>\n",
       "      <td>2181</td>\n",
       "      <td>2495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>495.119995</td>\n",
       "      <td>522.750000</td>\n",
       "      <td>494.790009</td>\n",
       "      <td>522.530029</td>\n",
       "      <td>522.505493</td>\n",
       "      <td>64251000</td>\n",
       "      <td>1.106481</td>\n",
       "      <td>3888</td>\n",
       "      <td>4302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2024-01-02  492.440002  492.950012  475.950012  481.679993  481.657410   \n",
       "1  2024-01-03  474.850006  481.839996  473.200012  475.690002  475.667694   \n",
       "2  2024-01-04  477.670013  485.000000  475.079987  479.980011  479.957489   \n",
       "3  2024-01-05  484.619995  495.470001  483.059998  490.970001  490.946960   \n",
       "4  2024-01-08  495.119995  522.750000  494.790009  522.530029  522.505493   \n",
       "\n",
       "     Volume  sen_mean  twt_volume  sen_sum  \n",
       "0  41125400  1.083774        1325     1436  \n",
       "1  32089600  1.102433        1562     1722  \n",
       "2  30653500  1.138872        1649     1878  \n",
       "3  41456800  1.143971        2181     2495  \n",
       "4  64251000  1.106481        3888     4302  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.merge(nvda_stock, sentiment_df, left_on = \"Date\", right_on = \"Time_Created\", how = 'inner')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stock prediction using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(stock, lookback):\n",
    "    data_raw = stock.to_numpy() # convert to numpy array\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - lookback): \n",
    "        data.append(data_raw[index: index + lookback])\n",
    "    \n",
    "    data = np.array(data);\n",
    "    test_set_size = int(np.round(0.2*data.shape[0]));\n",
    "    train_set_size = data.shape[0] - (test_set_size);\n",
    "    \n",
    "    x_train = data[:train_set_size,:-1,:]\n",
    "    y_train = data[:train_set_size,-1,:]\n",
    "    \n",
    "    x_test = data[train_set_size:,:-1]\n",
    "    y_test = data[train_set_size:,-1,:]\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "lookback = 20 # choose sequence length\n",
    "x_train, y_train, x_test, y_test = split_data(price, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train_lstm = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test_lstm = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "y_train_gru = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test_gru = torch.from_numpy(y_test).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "for t in range(num_epochs):\n",
    "    y_train_pred = model(x_train)\n",
    "    loss = criterion(y_train_pred, y_train_lstm)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "training_time = time.time()-start_time\n",
    "print(\"Training time: {}\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn) = self.gru(x, (h0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
